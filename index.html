<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daria Soboleva</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <div class="container">
        <header>
            <img src="daria.jpg" alt="Daria Soboleva" class="profile-img" style="width: 350px; height: 350px;">
            <h1>Daria Soboleva</h1>
            <p class="tagline">Head Research Scientist at Cerebras</p>
            <div class="social-links">
                <a href="https://github.com/soboleva-daria" title="GitHub"><i class="fab fa-github"></i></a>
                <a href="https://www.linkedin.com/in/daria-soboleva/" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
                <a href="https://scholar.google.com/citations?user=Oxv3_38AAAAJ&hl=en" title="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
                <a href="https://x.com/dmsobol" title="Twitter"><i class="fab fa-twitter"></i></a>
                <a href="https://www.youtube.com/@dmsobol" title="YouTube"><i class="fab fa-youtube"></i></a>
                <a href="mailto:dariamsoboleva@gmail.com" title="Email"><i class="fas fa-envelope"></i></a>
            </div>
        </header>

        <section id="bio">
            <p>I'm a Head Research Scientist at Cerebras with nearly a decade of AI/ML experience. I designed MoE recipes from the ground up for Cerebras hardware and now lead training at unprecedented scale across hardware, software, and ML teams. From this hands-on work, I distill practical MoE recipes into <a href="https://www.cerebras.ai/moe-guide">The MoE 101 Guide</a>.</p>
            <p>My focus is building efficient, scalable AI systems end to end, spanning data, models, and infrastructure. This includes <a href="https://huggingface.co/datasets/cerebras/SlimPajama-627B">SlimPajama</a>, a 627B-token dataset with over 1M downloads, and <a href="https://huggingface.co/cerebras/btlm-3b-8k-base">BTLM</a>, a 3B model achieving 7B-class quality while using 3× less inference compute.</p>
            <p>Previously, I built ML systems used by millions. At Yandex, I invented the <a href="https://habr.com/en/articles/694834/">YATI model</a> that powers Yandex Search. At Google, I focused on improving Google Assistant's ASR and built models that were deployed in Google Captions and Google Gboard.</p>
        </section>

        <section id="moe-guide">
            <h2>The MoE 101 Guide</h2>
            <p>Full course is available at: <a href="https://www.cerebras.ai/moe-guide">https://www.cerebras.ai/moe-guide</a></p>
            
            <div class="talks-grid">
                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=EUkOo7jWxgc&list=PLgyaY0hJJIdYrSLoZcEP6mA9JDWY-gJVY&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="https://img.youtube.com/vi/EUkOo7jWxgc/hqdefault.jpg" alt="MoE Fundamentals Talk">
                        </div>
                        <div class="talk-info">
                            <h3>MoE Fundamentals: Why Sparse Models Are the Future</h3>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=phXUzFt7hrs&list=PLgyaY0hJJIdbZ_JdUJo44fDcMA7xMZGli&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="moe_101_debug.jpg" alt="Debugging Dead MoE Models Talk">
                        </div>
                        <div class="talk-info">
                            <h3>Debugging Dead MoE Models: A Step-by-Step Guide</h3>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=MXo9LEYzwkg&list=PLgyaY0hJJIdauzJaVshynUSRuQ6lho0xJ&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="moe_101_atscale.jpg" alt="MoE at Scale Talk">
                        </div>
                        <div class="talk-info">
                            <h3>MoE at Scale: Making Sparse Models Fast on Real Hardware</h3>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=gHpDBoyCOrE" class="talk-link">
                        <div class="talk-image">
                            <img src="moe_101_math.jpg" alt="MoE Math Talk">
                        </div>
                        <div class="talk-info">
                            <h3>MoE Math Demystified: What Does 8x7B Actually Mean?</h3>
                        </div>
                    </a>
                </div>
            </div>
            <p style="margin-top: 1.5em;">I regularly advise top AI labs on MoE architecture and training dynamics. For consulting inquiries or early-stage collaborations, please reach out via email: <a href="mailto:dariamsoboleva@gmail.com">dariamsoboleva@gmail.com</a>.</p>
        </section>

        <section id="professional-services">
            <h2>Professional services</h2>
            
            <div class="talks-grid" style="grid-template-columns: repeat(3, 1fr);">
                <div class="talk-card">
                    <a href="https://youtube.com/playlist?list=PLgyaY0hJJIda2Dn1nt-0jrydHM0g6G6TE" class="talk-link">
                        <div class="talk-image">
                            <img src="8th-scaling-workshop-neurips.jpg" alt="8th Neural Scaling Workshop at NeurIPS 2025">
                        </div>
                        <div class="talk-info">
                            <h3>8th Neural Scaling Workshop</h3>
                            <p class="talk-venue">NeurIPS, 2025</p>
                        </div>
                    </a>
                </div>
            </div>
            <p style="margin-top: 1.5em;">Speaker, organizer, and moderator for both panels.</p>
        </section>

        <section id="talks">
            <h2>Featured talks</h2>
            
            <div class="talks-grid">
                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=zYvGVSMC6Js" class="talk-link">
                        <div class="talk-image">
                            <img src="https://img.youtube.com/vi/zYvGVSMC6Js/maxresdefault.jpg" alt="TNG Big TechDay Talk">
                        </div>
                        <div class="talk-info">
                            <h3>Sparse Models are the Future: A Deep Dive into Mixture-of-Experts</h3>
                            <p class="talk-venue">TNG Big TechDay, 2025</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=tzRvcTEapzo&list=PLgyaY0hJJIdb-vSME5c45HV69JBrfglRj&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="https://img.youtube.com/vi/tzRvcTEapzo/maxresdefault.jpg" alt="AI Engineer World's Fair Talk">
                        </div>
                        <div class="talk-info">
                            <h3>From Mixture of Experts to Mixture of Agents</h3>
                            <p class="talk-venue">AI Engineer World's Fair, 2025</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=Ziz-TzfLKcY&t=28m35s" class="talk-link">
                        <div class="talk-image">
                            <img src="panel happyverse agi summit.jpg" alt="Happyverse AGI Summit Panel">
                        </div>
                        <div class="talk-info">
                            <h3>AGI System Builders Panel</h3>
                            <p class="talk-venue">Happyverse AGI Summit, 2025</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=tHvJJ3ue9Hk&list=PLgyaY0hJJIdY_2UR8UsymqzsMZNCMyiMu&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="moe-101-talk.jpg" alt="MoE 101 Talk">
                        </div>
                        <div class="talk-info">
                            <h3>DeepSeek-V3 vs Qwen3</h3>
                            <p class="talk-venue">Cerebras Tech Talk, 2025</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=bUH5tvIZInE&list=PLgyaY0hJJIdb_FGSSNspSCSlxABtlBL_6&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="founders-inc-talk.jpg" alt="Founders Inc Talk">
                        </div>
                        <div class="talk-info">
                            <h3>MoE Crash Course: Realtime AI Applications</h3>
                            <p class="talk-venue">Founders Inc, 2025</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=zesfMnGRmFo&list=PLgyaY0hJJIdax3GngOqyAimZP-a-b0nhj&pp=gAQB0gcJCWMEOCosWNin" class="talk-link">
                        <div class="talk-image">
                            <img src="inference-con-talk.jpg" alt="InferenceCon Talk">
                        </div>
                        <div class="talk-info">
                            <h3>Accelerating LLM Inference with MoE</h3>
                            <p class="talk-venue">InferenceCon, 2025</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=HVXDPOwA-tg&list=PLgyaY0hJJIdaLsOD4Ya4A1lsKLaqtRtTX&pp=gAQB0gcJCWMEOCosWNin" class="talk-link">
                        <div class="talk-image">
                            <img src="neurips-2024-talk.jpg" alt="NeurIPS 2024 Talk">
                        </div>
                        <div class="talk-info">
                            <h3>We Grokked Sparse Mixture of Experts: Routing Metrics That Matter</h3>
                            <p class="talk-venue">NeurIPS, 2024</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=xEkEFD6UWvU&list=PLgyaY0hJJIdZfATTiXM06-7mgA8cdI6Wz&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="deployment-guide-talk.jpg" alt="Deployment Guide Talk">
                        </div>
                        <div class="talk-info">
                            <h3>Practitioner's Guide: Deploying Cerebras-GPT LLMs</h3>
                            <p class="talk-venue">Cerebras Developer Community, 2023</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=1bJzQLw6pWg&list=PLgyaY0hJJIdYNrlG3vJZYbWm3f4Z_cOYs" class="talk-link">
                        <div class="talk-image">
                            <img src="https://img.youtube.com/vi/1bJzQLw6pWg/maxresdefault.jpg" alt="Big Chip Club Podcast">
                        </div>
                        <div class="talk-info">
                            <h3>Why Mixture-of-Experts Took 30 Years to Take Off</h3>
                            <p class="talk-venue">Big Chip Club, 2026</p>
                        </div>
                    </a>
                </div>
            </div>
        </section>

        <section id="publications">
            <h2>Recent publications</h2>
            
            <ul class="publications-list">
                <li>
                    <strong>Daria Soboleva</strong>, Etienne Goffinet, Hui Zeng, Sangamesh Ragate, Elif Albuz, Natalia Vassilieva, <em>Batch Tiling on Attention: Efficient Mixture of Experts Training on Wafer-Scale Processors</em>, SC, 2025 
                    <a href="https://dl.acm.org/doi/full/10.1145/3731599.3767407">[paper]</a>
                </li>

                <li>
                    Krishna Teja Chitty-Venkata, Sylvia Howland, Golara Azar, <strong>Daria Soboleva</strong>, Natalia Vassilieva, Siddhisanket Raskar, Murali Emani, Venkatram Vishwanath, <em>MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models</em>, SC, 2025 
                    <a href="https://dl.acm.org/doi/full/10.1145/3731599.3767706">[paper]</a>
                </li>

                <li>
                    K2 Team (incl. <strong>Daria Soboleva</strong>), <em>K2-V2: A 360-Open, Reasoning-Enhanced LLM</em>, arXiv, 2025 
                    <a href="https://arxiv.org/abs/2512.06201">[paper]</a>
                </li>

                <li>
                    Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, <strong>Daria Soboleva</strong>, Joel Hestness, <em>Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training</em>, NeurIPS, 2025 
                    <a href="https://openreview.net/pdf?id=bFXbLQzRoZ">[paper]</a>
                </li>

                <li>
                    Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, <strong>Daria Soboleva</strong>, Joel Hestness, <em>Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs</em>, ICLR, 2025 
                    <a href="https://openreview.net/pdf?id=hrOlBgHsMI">[paper]</a>
                </li>

                <li>
                    <strong>Daria Soboleva</strong>, Faisal Al-Khateeb, Robert Myers, et al., <em>SlimPajama: A 627B token cleaned and deduplicated version of RedPajama</em>, 2023 
                    <a href="https://huggingface.co/datasets/cerebras/SlimPajama-627B">[dataset]</a>
                    <a href="https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama">[blog post]</a>
                    <a href="https://github.com/Cerebras/modelzoo/tree/main/src/cerebras/modelzoo/data_preparation/nlp/slimpajama">[github]</a>
                </li>

                <li>
                    Nolan Dey*, <strong>Daria Soboleva*</strong>, Faisal Al-Khateeb, et al., <em>BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model</em>, Efficient Natural Language and Speech Processing, NeurIPS, 2023 (* Equal contribution) 
                    <a href="https://huggingface.co/cerebras/btlm-3b-8k-base">[model]</a>
                    <a href="https://neurips2023-enlsp.github.io/papers/paper_45.pdf">[paper]</a>
                    <a href="https://dblalock.substack.com/p/2023-9-arxiv-roundup-a-bunch-of-good">[Davis Bialock's roundup]</a>
                    <a href="https://www.latent.space/p/fastai#details">[Jeremy Howard (fast.ai)]</a>
                </li>

                <li>
                    Faisal Al-Khateeb, Nolan Dey, <strong>Daria Soboleva</strong>, Joel Hestness, <em>Position Interpolation Improves ALiBi Extrapolation</em>, arXiv, 2023 
                    <a href="https://arxiv.org/abs/2310.13017">[paper]</a>
                </li>

                <li>
                    Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, <strong>Daria Soboleva</strong>, Eric Xing, <em>SlimPajama-DC: Understanding Data Combinations for LLM Training</em>, arXiv, 2023 
                    <a href="https://arxiv.org/abs/2309.10818">[paper]</a>
                </li>

                <li>
                    <strong>Daria Soboleva</strong>, Ondrej Skopek, Márius Šajgalík, et al., <em>Replacing Human Audio with Synthetic Audio for On-device Unspoken Punctuation Prediction</em>, ICASSP, 2021 
                    <a href="https://ieeexplore.ieee.org/abstract/document/9413432">[paper]</a>
                </li>
            </ul>

            <h2>Patents</h2>

            <ul class="publications-list">
                <li>
                    Aleksandr Boymel, <strong>Daria Soboleva</strong>, <em>Multi-phase training of machine learning models for search ranking</em>, Filed, 2022 (Yandex) 
                    <a href="https://patents.google.com/patent/US20230177097A1/en">[patent]</a>
                </li>
            </ul>

            <p class="scholar-link">Also on <a href="https://scholar.google.com/citations?user=Oxv3_38AAAAJ&hl=en">Google Scholar</a></p>
        </section>

        <footer>
            <p>© 2025 Daria Soboleva</p>
        </footer>
    </div>
</body>
</html>
