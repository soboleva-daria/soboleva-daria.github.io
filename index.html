<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daria Soboleva</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <div class="container">
        <header>
            <img src="daria.jpg" alt="Daria Soboleva" class="profile-img" style="width: 350px; height: 350px;">
            <h1>Daria Soboleva</h1>
            <p class="tagline">Head Research Scientist at Cerebras</p>
            <div class="social-links">
                <a href="https://github.com/soboleva-daria" title="GitHub"><i class="fab fa-github"></i></a>
                <a href="https://www.linkedin.com/in/daria-soboleva/" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
                <a href="https://scholar.google.com/citations?user=Oxv3_38AAAAJ&hl=en" title="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
                <a href="https://x.com/dmsobol" title="Twitter"><i class="fab fa-twitter"></i></a>
                <a href="https://www.youtube.com/@dmsobol" title="YouTube"><i class="fab fa-youtube"></i></a>
                <a href="mailto:dariamsoboleva@gmail.com" title="Email"><i class="fas fa-envelope"></i></a>
            </div>
        </header>

        <section id="bio">
            <p>I'm a Head Research Scientist at Cerebras with almost a decade of AI/ML experience. I created <a href="https://www.cerebras.ai/moe-guide">The MoE 101 Guide</a>, a series that teaches you how to build MoE systems that actually work, not just read about them in papers. I focus on building efficient AI systems and making cutting-edge research more accessible to everyone. My work includes <a href="https://huggingface.co/datasets/cerebras/SlimPajama-627B">SlimPajama</a>, a 627B token dataset that achieved more than 1M downloads, and <a href="https://huggingface.co/cerebras/btlm-3b-8k-base">BTLM</a>, a 3B model comparable to 7B models in quality while using 3x less inference compute. Previously, I had fun building ML systems that millions use daily: invented the <a href="https://yandex.com/company/blog?tag=yac">YATI model</a> that powers Yandex Search. At Google, I focused on improving Google Assistant's ASR and built models that were deployed in Google Captions and Google Gboard. I have an MS in Computer Science from Moscow State University.</p>
        </section>

        <section id="moe-guide">
            <h2>The MoE 101 Guide</h2>
            <p>Full course is available at: <a href="https://www.cerebras.ai/moe-guide">https://www.cerebras.ai/moe-guide</a></p>
            
            <div class="talks-grid">
                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=EUkOo7jWxgc&list=PLgyaY0hJJIdYrSLoZcEP6mA9JDWY-gJVY&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="https://img.youtube.com/vi/EUkOo7jWxgc/hqdefault.jpg" alt="MoE Fundamentals Talk">
                        </div>
                        <div class="talk-info">
                            <h3>MoE Fundamentals: Why Sparse Models Are the Future</h3>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=phXUzFt7hrs&list=PLgyaY0hJJIdbZ_JdUJo44fDcMA7xMZGli&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="https://img.youtube.com/vi/phXUzFt7hrs/maxresdefault.jpg" alt="Debugging Dead MoE Models Talk">
                        </div>
                        <div class="talk-info">
                            <h3>Debugging Dead MoE Models: A Step-by-Step Guide</h3>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=MXo9LEYzwkg&list=PLgyaY0hJJIdauzJaVshynUSRuQ6lho0xJ&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="https://img.youtube.com/vi/MXo9LEYzwkg/maxresdefault.jpg" alt="MoE at Scale Talk">
                        </div>
                        <div class="talk-info">
                            <h3>MoE at Scale: Making Sparse Models Fast on Real Hardware</h3>
                        </div>
                    </a>
                </div>
            </div>
        </section>

        <section id="talks">
            <h2>Featured talks</h2>
            
            <div class="talks-grid">
                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=tzRvcTEapzo&list=PLgyaY0hJJIdb-vSME5c45HV69JBrfglRj&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="https://img.youtube.com/vi/tzRvcTEapzo/maxresdefault.jpg" alt="AI Engineer World's Fair Talk">
                        </div>
                        <div class="talk-info">
                            <h3>From Mixture of Experts to Mixture of Agents</h3>
                            <p class="talk-venue">AI Engineer World's Fair, 2025</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=tHvJJ3ue9Hk&list=PLgyaY0hJJIdY_2UR8UsymqzsMZNCMyiMu&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="moe-101-talk.jpg" alt="MoE 101 Talk">
                        </div>
                        <div class="talk-info">
                            <h3>MoE 101: DeepSeek-V3 vs Qwen3</h3>
                            <p class="talk-venue">Cerebras Tech Talk, 2025</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=bUH5tvIZInE&list=PLgyaY0hJJIdb_FGSSNspSCSlxABtlBL_6&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="founders-inc-talk.jpg" alt="Founders Inc Talk">
                        </div>
                        <div class="talk-info">
                            <h3>MoE Crash Course: Realtime AI Applications</h3>
                            <p class="talk-venue">Founders Inc, 2025</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=zesfMnGRmFo&list=PLgyaY0hJJIdax3GngOqyAimZP-a-b0nhj&pp=gAQB0gcJCWMEOCosWNin" class="talk-link">
                        <div class="talk-image">
                            <img src="inference-con-talk.jpg" alt="InferenceCon Talk">
                        </div>
                        <div class="talk-info">
                            <h3>Accelerating LLM Inference with MoE</h3>
                            <p class="talk-venue">InferenceCon, 2025</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=HVXDPOwA-tg&list=PLgyaY0hJJIdaLsOD4Ya4A1lsKLaqtRtTX&pp=gAQB0gcJCWMEOCosWNin" class="talk-link">
                        <div class="talk-image">
                            <img src="neurips-2024-talk.jpg" alt="NeurIPS 2024 Talk">
                        </div>
                        <div class="talk-info">
                            <h3>We Grokked Sparse Mixture of Experts: Routing Metrics That Matter</h3>
                            <p class="talk-venue">NeurIPS, 2024</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=srOJb__esyQ&list=PLgyaY0hJJIdbPlQNR4RNez8UY0WLLrUlv&pp=gAQB0gcJCWMEOCosWNin" class="talk-link">
                        <div class="talk-image">
                            <img src="cerebras-gpu-panel.jpg" alt="Cerebras vs GPUs Panel">
                        </div>
                        <div class="talk-info">
                            <h3>Cerebras vs GPUs: Specialized AI Hardware Benefits</h3>
                            <p class="talk-venue">Simuli, 2023</p>
                        </div>
                    </a>
                </div>

                <div class="talk-card">
                    <a href="https://www.youtube.com/watch?v=xEkEFD6UWvU&list=PLgyaY0hJJIdZfATTiXM06-7mgA8cdI6Wz&pp=gAQB" class="talk-link">
                        <div class="talk-image">
                            <img src="deployment-guide-talk.jpg" alt="Deployment Guide Talk">
                        </div>
                        <div class="talk-info">
                            <h3>Practitioner's Guide: Deploying Cerebras-GPT LLMs</h3>
                            <p class="talk-venue">Cerebras Developer Community, 2023</p>
                        </div>
                    </a>
                </div>
            </div>
        </section>

        <section id="publications">
            <h2>Recent publications</h2>
            
            <ul class="publications-list">
                <li>
                    Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, <strong>Daria Soboleva</strong>, Joel Hestness, <em>Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training</em>, Under review (arXiv, 2025) 
                    <a href="https://arxiv.org/abs/2505.13738">[paper]</a>
                </li>

                <li>
                    Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, <strong>Daria Soboleva</strong>, Joel Hestness, <em>Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs</em>, ICLR, 2025 
                    <a href="https://openreview.net/pdf?id=hrOlBgHsMI">[paper]</a>
                </li>

                <li>
                    <strong>Daria Soboleva</strong>, Faisal Al-Khateeb, Robert Myers, et al., <em>SlimPajama: A 627B token cleaned and deduplicated version of RedPajama</em>, 2023 
                    <a href="https://huggingface.co/datasets/cerebras/SlimPajama-627B">[dataset]</a>
                    <a href="https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama">[blog post]</a>
                    <a href="https://github.com/Cerebras/modelzoo/tree/main/src/cerebras/modelzoo/data_preparation/nlp/slimpajama">[github]</a>
                </li>

                <li>
                    Nolan Dey*, <strong>Daria Soboleva*</strong>, Faisal Al-Khateeb, et al., <em>BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model</em>, Efficient Natural Language and Speech Processing, NeurIPS, 2023 (* Equal contribution) 
                    <a href="https://huggingface.co/cerebras/btlm-3b-8k-base">[model]</a>
                    <a href="https://neurips2023-enlsp.github.io/papers/paper_45.pdf">[paper]</a>
                    <a href="https://dblalock.substack.com/p/2023-9-arxiv-roundup-a-bunch-of-good">[Davis Bialock's roundup]</a>
                    <a href="https://www.latent.space/p/fastai#details">[Jeremy Howard (fast.ai)]</a>
                </li>

                <li>
                    Faisal Al-Khateeb, Nolan Dey, <strong>Daria Soboleva</strong>, Joel Hestness, <em>Position Interpolation Improves ALiBi Extrapolation</em>, arXiv, 2023 
                    <a href="https://arxiv.org/abs/2310.13017">[paper]</a>
                </li>

                <li>
                    Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, <strong>Daria Soboleva</strong>, Eric Xing, <em>SlimPajama-DC: Understanding Data Combinations for LLM Training</em>, arXiv, 2023 
                    <a href="https://arxiv.org/abs/2309.10818">[paper]</a>
                </li>

                <li>
                    <strong>Daria Soboleva</strong>, Ondrej Skopek, Márius Šajgalík, et al., <em>Replacing Human Audio with Synthetic Audio for On-device Unspoken Punctuation Prediction</em>, ICASSP, 2021 
                    <a href="https://ieeexplore.ieee.org/abstract/document/9413432">[paper]</a>
                </li>
            </ul>

            <h3 class="subsection-title">Patents</h3>

            <ul class="publications-list">
                <li>
                    Aleksandr Boymel, <strong>Daria Soboleva</strong>, <em>Multi-phase training of machine learning models for search ranking</em>, Filed, 2022 (Yandex) 
                    <a href="https://patents.google.com/patent/US20230177097A1/en">[patent]</a>
                </li>
            </ul>

            <p class="scholar-link">Also on <a href="https://scholar.google.com/citations?user=Oxv3_38AAAAJ&hl=en">Google Scholar</a></p>
        </section>

        <footer>
            <p>© 2025 Daria Soboleva</p>
        </footer>
    </div>
</body>
</html>
